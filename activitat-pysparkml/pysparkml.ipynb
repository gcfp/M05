{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llibreria ML de PySpark\n",
    "Part 1. Pràctica\n",
    "Descarrega: https://archive.ics.uci.edu/ml/datasets/covertype\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Carrega l’arxiu a un spark dataframe directament i carrega l’arxiu passant per un pas intermig de crear un format parquet i després el spark dataframe. Són els 2 dataframes iguals? Sí\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicialitzem spark\n",
    "findspark.init()\n",
    "\n",
    "# Creem una sessió de spark\n",
    "spark = SparkSession.builder.appName('pysparkml').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Afegim les capçaleres del dataset\n",
    "header = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Cover_Type']\n",
    "\n",
    "# Carreguem el dataset a un spark dataframe amb les capçaleres\n",
    "covertype_df = spark.read.csv('covtype.data', header=False, inferSchema=True)\n",
    "covertype_df = covertype_df.toDF(*header)\n",
    "\n",
    "# Convertim a parquet\n",
    "#covertype_df.write.parquet('covertype.parquet', mode='overwrite')\n",
    "#covertype_parquet_df = spark.read.parquet()\n",
    "\n",
    "# Comprovem que els dos dataframes són iguals\n",
    "#covertype_df.count() == covertype_parquet_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Elimina un dels 2 dataframe i realitza un EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wilderness_Area1: integer (nullable = true)\n",
      " |-- Wilderness_Area2: integer (nullable = true)\n",
      " |-- Wilderness_Area3: integer (nullable = true)\n",
      " |-- Wilderness_Area4: integer (nullable = true)\n",
      " |-- Soil_Type1: integer (nullable = true)\n",
      " |-- Soil_Type2: integer (nullable = true)\n",
      " |-- Soil_Type3: integer (nullable = true)\n",
      " |-- Soil_Type4: integer (nullable = true)\n",
      " |-- Soil_Type5: integer (nullable = true)\n",
      " |-- Soil_Type6: integer (nullable = true)\n",
      " |-- Soil_Type7: integer (nullable = true)\n",
      " |-- Soil_Type8: integer (nullable = true)\n",
      " |-- Soil_Type9: integer (nullable = true)\n",
      " |-- Soil_Type10: integer (nullable = true)\n",
      " |-- Soil_Type11: integer (nullable = true)\n",
      " |-- Soil_Type12: integer (nullable = true)\n",
      " |-- Soil_Type13: integer (nullable = true)\n",
      " |-- Soil_Type14: integer (nullable = true)\n",
      " |-- Soil_Type15: integer (nullable = true)\n",
      " |-- Soil_Type16: integer (nullable = true)\n",
      " |-- Soil_Type17: integer (nullable = true)\n",
      " |-- Soil_Type18: integer (nullable = true)\n",
      " |-- Soil_Type19: integer (nullable = true)\n",
      " |-- Soil_Type20: integer (nullable = true)\n",
      " |-- Soil_Type21: integer (nullable = true)\n",
      " |-- Soil_Type22: integer (nullable = true)\n",
      " |-- Soil_Type23: integer (nullable = true)\n",
      " |-- Soil_Type24: integer (nullable = true)\n",
      " |-- Soil_Type25: integer (nullable = true)\n",
      " |-- Soil_Type26: integer (nullable = true)\n",
      " |-- Soil_Type27: integer (nullable = true)\n",
      " |-- Soil_Type28: integer (nullable = true)\n",
      " |-- Soil_Type29: integer (nullable = true)\n",
      " |-- Soil_Type30: integer (nullable = true)\n",
      " |-- Soil_Type31: integer (nullable = true)\n",
      " |-- Soil_Type32: integer (nullable = true)\n",
      " |-- Soil_Type33: integer (nullable = true)\n",
      " |-- Soil_Type34: integer (nullable = true)\n",
      " |-- Soil_Type35: integer (nullable = true)\n",
      " |-- Soil_Type36: integer (nullable = true)\n",
      " |-- Soil_Type37: integer (nullable = true)\n",
      " |-- Soil_Type38: integer (nullable = true)\n",
      " |-- Soil_Type39: integer (nullable = true)\n",
      " |-- Soil_Type40: integer (nullable = true)\n",
      " |-- Cover_Type: integer (nullable = true)\n",
      "\n",
      "+-------+-----------------+------------------+------------------+--------------------------------+------------------------------+-------------------------------+------------------+------------------+------------------+----------------------------------+------------------+--------------------+-------------------+-------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|summary|        Elevation|            Aspect|             Slope|Horizontal_Distance_To_Hydrology|Vertical_Distance_To_Hydrology|Horizontal_Distance_To_Roadways|     Hillshade_9am|    Hillshade_Noon|     Hillshade_3pm|Horizontal_Distance_To_Fire_Points|  Wilderness_Area1|    Wilderness_Area2|   Wilderness_Area3|   Wilderness_Area4|          Soil_Type1|         Soil_Type2|          Soil_Type3|          Soil_Type4|          Soil_Type5|          Soil_Type6|          Soil_Type7|          Soil_Type8|          Soil_Type9|         Soil_Type10|        Soil_Type11|        Soil_Type12|         Soil_Type13|         Soil_Type14|         Soil_Type15|        Soil_Type16|         Soil_Type17|         Soil_Type18|         Soil_Type19|         Soil_Type20|         Soil_Type21|        Soil_Type22|        Soil_Type23|        Soil_Type24|         Soil_Type25|         Soil_Type26|         Soil_Type27|         Soil_Type28|        Soil_Type29|        Soil_Type30|        Soil_Type31|        Soil_Type32|        Soil_Type33|         Soil_Type34|         Soil_Type35|         Soil_Type36|         Soil_Type37|         Soil_Type38|        Soil_Type39|        Soil_Type40|        Cover_Type|\n",
      "+-------+-----------------+------------------+------------------+--------------------------------+------------------------------+-------------------------------+------------------+------------------+------------------+----------------------------------+------------------+--------------------+-------------------+-------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|  count|           581012|            581012|            581012|                          581012|                        581012|                         581012|            581012|            581012|            581012|                            581012|            581012|              581012|             581012|             581012|              581012|             581012|              581012|              581012|              581012|              581012|              581012|              581012|              581012|              581012|             581012|             581012|              581012|              581012|              581012|             581012|              581012|              581012|              581012|              581012|              581012|             581012|             581012|             581012|              581012|              581012|              581012|              581012|             581012|             581012|             581012|             581012|             581012|              581012|              581012|              581012|              581012|              581012|             581012|             581012|            581012|\n",
      "|   mean|2959.365300544567|155.65680743254873|14.103703537964792|              269.42821662891646|            46.418855376481034|             2350.1466114297123|212.14604861861716|223.31871630878535|142.52826275533036|                1980.2912263430014|0.4488650836815763|0.051434393781884025| 0.4360736094951567|0.06362691304138296|0.005216759722690753|0.01295153972723455|0.008301033369362424|0.021335187569275677|0.002748652351414...|0.011316461622135171|1.807191589846681...|3.080831376976723...|0.001974141670051565|0.056167514612434855| 0.0213592834571403|0.05158413251361418|0.030001101526302382|0.001030959773636...|5.163404542419089E-6|0.00489662864106077|0.005889723448052708|0.003268435075351...|0.006920683221689053|0.015935987552752783|0.001442311002182399|0.05743943326471743|0.09939897971126242|0.03662230728453113|8.158179177022161E-4|0.004456018120107674|0.001869152444355...|0.001628193565709...|0.19835562776672427|0.05192663834826131|0.04417464699524278|0.09039228105443606|0.07771612290279718|0.002772748239279...|0.003254665996571...|2.048150468492905...|5.128981845469629E-4|0.026803232979697493|0.02376198770421265|0.01505992991538901| 2.051470537613681|\n",
      "| stddev|279.9847342506371|111.91372100329544| 7.488241814480139|              212.54935559508115|             58.29523162688723|             1559.2548698976084| 26.76988880528213|19.768697153666405| 38.27452923141059|                1324.1952097801075|0.4973787752982266|   0.220882278353401|0.49589700528224817|0.24408734404115698| 0.07203855962336306|0.11306555332463286| 0.09073114340173861| 0.14449925009498288| 0.05235553437289732|  0.1057753212014363|0.013441979413635834|0.017549608330903396| 0.04438747374857372| 0.23024512189599608|0.14457904566150795|0.22118610713609077|  0.1705904027792983| 0.03209203434149953|0.002272308686899177|0.06980444151712224| 0.07651826372232713| 0.05707677298648149| 0.08290234733979654| 0.12522802739167171| 0.03795040474080564|0.23268054915182676|0.29919722027093215|0.18783283689852426|0.028550897740997946| 0.06660457685375018| 0.04319330879373808|0.040318052398458246|0.39876176644617517|0.22187912770284737|0.20548313855427747| 0.2867431918891414|0.26772457957496393|  0.0525838840856219| 0.05695681459947709| 0.01430990741020341| 0.02264146643472847|  0.1615080944631041|0.15230691241832534|0.12179143630071344|1.3965043160794064|\n",
      "|    min|             1859|                 0|                 0|                               0|                          -173|                              0|                 0|                 0|                 0|                                 0|                 0|                   0|                  0|                  0|                   0|                  0|                   0|                   0|                   0|                   0|                   0|                   0|                   0|                   0|                  0|                  0|                   0|                   0|                   0|                  0|                   0|                   0|                   0|                   0|                   0|                  0|                  0|                  0|                   0|                   0|                   0|                   0|                  0|                  0|                  0|                  0|                  0|                   0|                   0|                   0|                   0|                   0|                  0|                  0|                 1|\n",
      "|    max|             3858|               360|                66|                            1397|                           601|                           7117|               254|               254|               254|                              7173|                 1|                   1|                  1|                  1|                   1|                  1|                   1|                   1|                   1|                   1|                   1|                   1|                   1|                   1|                  1|                  1|                   1|                   1|                   1|                  1|                   1|                   1|                   1|                   1|                   1|                  1|                  1|                  1|                   1|                   1|                   1|                   1|                  1|                  1|                  1|                  1|                  1|                   1|                   1|                   1|                   1|                   1|                  1|                  1|                 7|\n",
      "+-------+-----------------+------------------+------------------+--------------------------------+------------------------------+-------------------------------+------------------+------------------+------------------+----------------------------------+------------------+--------------------+-------------------+-------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminem el dataframe que no utilitzarem\n",
    "#covertype_parquet_df.unpersist()\n",
    "\n",
    "# Realitzem un EDA general del dataframe\n",
    "covertype_df.printSchema()\n",
    "covertype_df.describe().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Elimina duplicats si n’hi ha. Busca outliers i, si n’hi ha, tracta’ls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hi ha duplicats\n",
      "+----------+-----+\n",
      "|Cover_Type|count|\n",
      "+----------+-----+\n",
      "|         1|20920|\n",
      "|         6| 1786|\n",
      "|         3| 3536|\n",
      "|         5|  934|\n",
      "|         4|  268|\n",
      "|         7| 2004|\n",
      "|         2|28242|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|Cover_Type|count|\n",
      "+----------+-----+\n",
      "|         1| 2228|\n",
      "|         6| 1786|\n",
      "|         3| 1725|\n",
      "|         5|  934|\n",
      "|         4|  268|\n",
      "|         7| 2004|\n",
      "|         2| 2918|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agafem un sample del dataframe per a fer un EDA més rapid\n",
    "covertype_df = covertype_df.sample(False, 0.1, seed=0)\n",
    "\n",
    "# Eliminem duplicats\n",
    "covertype_df = covertype_df.dropDuplicates()\n",
    "\n",
    "# Comprovem que no hi ha duplicats\n",
    "if covertype_df.count() == covertype_df.dropDuplicates().count():\n",
    "    print('No hi ha duplicats')\n",
    "else:\n",
    "    print('Hi ha duplicats')\n",
    "\n",
    "# Eliminem nuls\n",
    "covertype_df = covertype_df.na.drop()\n",
    "\n",
    "# Nivellem la quantitat de Cover_Type per a que no hi hagi desequilibri en el model.\n",
    "covertype_df.groupBy('Cover_Type').count().show()\n",
    "covertype_df = covertype_df.sampleBy('Cover_Type', fractions={1: 0.1, 2: 0.1, 3: 0.5, 4: 1, 5: 1, 6: 1, 7: 1}, seed=0)\n",
    "covertype_df.groupBy('Cover_Type').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tractant: Elevation\n",
      "Outliers restants:  0\n",
      "Tractant: Aspect\n",
      "Outliers restants:  0\n",
      "Tractant: Slope\n",
      "Outliers restants:  0\n",
      "Tractant: Horizontal_Distance_To_Hydrology\n",
      "Outliers restants:  115\n",
      "Tractant: Vertical_Distance_To_Hydrology\n",
      "Outliers restants:  121\n",
      "Tractant: Horizontal_Distance_To_Roadways\n",
      "Outliers restants:  110\n",
      "Tractant: Hillshade_9am\n",
      "Outliers restants:  76\n",
      "Tractant: Hillshade_Noon\n",
      "Outliers restants:  120\n",
      "Tractant: Hillshade_3pm\n",
      "Outliers restants:  0\n",
      "Tractant: Horizontal_Distance_To_Fire_Points\n",
      "Outliers restants:  101\n",
      "Tractant: Wilderness_Area1\n",
      "Outliers restants:  0\n",
      "Tractant: Wilderness_Area2\n",
      "Outliers restants:  0\n",
      "Tractant: Wilderness_Area3\n",
      "Outliers restants:  0\n",
      "Tractant: Wilderness_Area4\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type1\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type2\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type3\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type4\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type5\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type6\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type7\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type8\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type9\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type10\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type11\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type12\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type13\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type14\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type15\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type16\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type17\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type18\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type19\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type20\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type21\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type22\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type23\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type24\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type25\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type26\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type27\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type28\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type29\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type30\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type31\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type32\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type33\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type34\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type35\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type36\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type37\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type38\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type39\n",
      "Outliers restants:  0\n",
      "Tractant: Soil_Type40\n",
      "Outliers restants:  0\n",
      "Tractant: Cover_Type\n",
      "Outliers restants:  0\n"
     ]
    }
   ],
   "source": [
    "# Tractador d'outliers\n",
    "def treat_outliers(df, col, search):\n",
    "    # Calculem el quartil inferior\n",
    "    q1 = df.approxQuantile(col, [0.25], 0.0)[0]\n",
    "    # Calculem el quartil superior\n",
    "    q3 = df.approxQuantile(col, [0.75], 0.0)[0]\n",
    "    # Calculem el rang interquartil\n",
    "    iqr = q3 - q1\n",
    "    # Calculem el límit inferior\n",
    "    lower_bound = q1 - (1.5 * iqr)\n",
    "    # Calculem el límit superior\n",
    "    upper_bound = q3 + (1.5 * iqr)\n",
    "    # Retornem el dataframe sense outliers\n",
    "    if search:\n",
    "        return df.filter((df[col] < lower_bound) | (df[col] > upper_bound))\n",
    "    else:\n",
    "        return df.filter((df[col] >= lower_bound) & (df[col] <= upper_bound))\n",
    "\n",
    "for col in covertype_df.columns:\n",
    "    # Tractem els outliers de cada columna\n",
    "    print(\"Tractant:\", col)\n",
    "    covertype_df = treat_outliers(covertype_df, col, False)\n",
    "    # Comprovem que no hi ha outliers\n",
    "    print(\"Outliers restants: \",treat_outliers(covertype_df, col, True).count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Problema de classificació de la variable COVER TYPE amb algoritme Regressió Logística del modul ML de PySpark.\n",
    "    Fixa’t en com es fa per agrupar i manipular les features a Spark.\n",
    "    Fixa’t en la transformació VectorAssembler.\n",
    "    Fixa’t amb la transformació Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6142857142857143\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.classification as cl\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Creem un vector de features\n",
    "feature_cols = covertype_df.columns[:-1]\n",
    "\n",
    "# Creem un objecte de la classe VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "\n",
    "# Creem un objecte de la classe LogisticRegression\n",
    "lr = cl.LogisticRegression(featuresCol='features', labelCol='Cover_Type', maxIter=10)\n",
    "\n",
    "# Creem un objecte de la classe Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "# Dividim el dataset en train i test\n",
    "train, test = covertype_df.randomSplit([0.7, 0.3], seed=1234)\n",
    "\n",
    "# Entrenem el model\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Fem prediccions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Mostrem l'accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Cover_Type', predictionCol='prediction', metricName='accuracy')\n",
    "print('Accuracy: ', evaluator.evaluate(predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Problema de classificació de la variable COVER TYPE amb algoritme Random forest amb modul ML de PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6761904761904762\n"
     ]
    }
   ],
   "source": [
    "# Creem un objecte de la classe RandomForestClassifier\n",
    "rf = cl.RandomForestClassifier(featuresCol='features', labelCol='Cover_Type', numTrees=10)\n",
    "\n",
    "# Creem un objecte de la classe Pipeline\n",
    "pipeline_rf = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Entrenem el model\n",
    "model_rf = pipeline_rf.fit(train)\n",
    "\n",
    "# Fem prediccions\n",
    "predictions_rf = model_rf.transform(test)\n",
    "\n",
    "# Mostrem l'accuracy\n",
    "print('Accuracy: ', evaluator.evaluate(predictions_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Comparativa de les métriques d’error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "MSE:  4.714285714285714\n",
      "RMSE:  2.1712405933672376\n",
      "R2:  0.30802585167489194\n",
      "MAE:  1.0809523809523809\n",
      "Random Forest\n",
      "MSE:  3.3833333333333333\n",
      "RMSE:  1.8393839548428526\n",
      "R2:  0.5033862299141523\n",
      "MAE:  0.8214285714285714\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Creem els evaluadors\n",
    "evaluator_mse = RegressionEvaluator(labelCol='Cover_Type', predictionCol='prediction', metricName='mse')\n",
    "evaluator_rmse = RegressionEvaluator(labelCol='Cover_Type', predictionCol='prediction', metricName='rmse')\n",
    "evaluator_r2 = RegressionEvaluator(labelCol='Cover_Type', predictionCol='prediction', metricName='r2')\n",
    "evaluator_mae = RegressionEvaluator(labelCol='Cover_Type', predictionCol='prediction', metricName='mae')\n",
    "\n",
    "# Comparem les métriques d'error dels dos models\n",
    "print('Logistic Regression')\n",
    "print('MSE: ', evaluator_mse.evaluate(predictions))\n",
    "print('RMSE: ', evaluator_rmse.evaluate(predictions))\n",
    "print('R2: ', evaluator_r2.evaluate(predictions))\n",
    "print('MAE: ', evaluator_mae.evaluate(predictions))\n",
    "print('Random Forest')\n",
    "print('MSE: ', evaluator_mse.evaluate(predictions_rf))\n",
    "print('RMSE: ', evaluator_rmse.evaluate(predictions_rf))\n",
    "print('R2: ', evaluator_r2.evaluate(predictions_rf))\n",
    "print('MAE: ', evaluator_mae.evaluate(predictions_rf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. «Hyperparameters-tuning» amb modul ML.tuning de PySpark\n",
    "Necessites fer: import pyspark.ml.tuning\n",
    "Es proposa fer un «grid» al problema de classificació amb Regressió Logística.\n",
    "Fixa’t en regParam i en elasticNetParam per afegir rangs al objecte del «grid»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6190476190476191\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.tuning\n",
    "\n",
    "# Creem un objecte de la classe regParam\n",
    "regParam = pyspark.ml.tuning.ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1, 0.5]).build()\n",
    "\n",
    "# Creem un objecte de la classe elasticNetParam\n",
    "elasticNetParam = pyspark.ml.tuning.ParamGridBuilder().addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build()\n",
    "\n",
    "# Creem un objecte de la classe ParamGridBuilder\n",
    "grid = pyspark.ml.tuning.ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1, 0.5]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build()\n",
    "\n",
    "# Creem un objecte de la classe CrossValidator\n",
    "crossvalidator = pyspark.ml.tuning.CrossValidator(estimator=pipeline, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Entrenem el model\n",
    "model_cv = crossvalidator.fit(train)\n",
    "\n",
    "# Fem prediccions\n",
    "predictions_cv = model_cv.transform(test)\n",
    "\n",
    "# Mostrem l'accuracy\n",
    "print('Accuracy: ', evaluator.evaluate(predictions_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Problema de regressió de la variable ELEVATION amb algoritme Random forest regressor del modul ML de PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.regression as reg\n",
    "\n",
    "# Creem un objecte de la classe RandomForestRegressor\n",
    "rf = reg.RandomForestRegressor(featuresCol='features', labelCol='Elevation')\n",
    "\n",
    "# Creem un objecte de la classe Pipeline\n",
    "pipeline_rf = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Entrenem el model\n",
    "model_rf = pipeline_rf.fit(train)\n",
    "\n",
    "# Fem prediccions\n",
    "predictions_rf = model_rf.transform(test)\n",
    "\n",
    "# Mostrem l'accuracy\n",
    "print('Accuracy: ', evaluator.evaluate(predictions_rf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Problema de regressió de la variable ELEVATION amb algoritme Gradient-boosted Tree regressor del modul ML de PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Creem un objecte de la classe GBTRegressor\n",
    "gbt = reg.GBTRegressor(featuresCol='features', labelCol='Elevation')\n",
    "\n",
    "# Creem un objecte de la classe Pipeline\n",
    "pipeline_gbt = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "# Entrenem el model\n",
    "model_gbt = pipeline_gbt.fit(train)\n",
    "\n",
    "# Fem prediccions\n",
    "predictions_gbt = model_gbt.transform(test)\n",
    "\n",
    "# Mostrem l'accuracy\n",
    "print('Accuracy: ', evaluator.evaluate(predictions_gbt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Comparativa de les métriques d’error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "MSE:  9664561.638649119\n",
      "RMSE:  3108.7878085596517\n",
      "R2:  -1418586.504905398\n",
      "MAE:  3103.0350121769266\n",
      "Gradient Boosted Trees\n",
      "MSE:  9686865.338480964\n",
      "RMSE:  3112.372943347401\n",
      "R2:  -1421860.2953862916\n",
      "MAE:  3105.2195392652948\n"
     ]
    }
   ],
   "source": [
    "# Comparem les métriques d'error dels dos models\n",
    "print('Random Forest')\n",
    "print('MSE: ', evaluator_mse.evaluate(predictions_rf))\n",
    "print('RMSE: ', evaluator_rmse.evaluate(predictions_rf))\n",
    "print('R2: ', evaluator_r2.evaluate(predictions_rf))\n",
    "print('MAE: ', evaluator_mae.evaluate(predictions_rf))\n",
    "print('Gradient Boosted Trees')\n",
    "print('MSE: ', evaluator_mse.evaluate(predictions_gbt))\n",
    "print('RMSE: ', evaluator_rmse.evaluate(predictions_gbt))\n",
    "print('R2: ', evaluator_r2.evaluate(predictions_gbt))\n",
    "print('MAE: ', evaluator_mae.evaluate(predictions_gbt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Clustering\n",
    "import pyspark.ml.clustering\n",
    "Fer un k-means de 7 centroides del mateix DataFrame del inici de la pràctica, què observes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|Cover_Type|prediction|\n",
      "+----------+----------+\n",
      "|         1|         1|\n",
      "|         2|         6|\n",
      "|         1|         2|\n",
      "|         7|         0|\n",
      "|         1|         0|\n",
      "|         2|         5|\n",
      "|         2|         2|\n",
      "|         2|         6|\n",
      "|         1|         4|\n",
      "|         7|         5|\n",
      "|         7|         3|\n",
      "|         7|         3|\n",
      "|         7|         3|\n",
      "|         7|         0|\n",
      "|         2|         6|\n",
      "|         2|         6|\n",
      "|         2|         4|\n",
      "|         2|         5|\n",
      "|         1|         3|\n",
      "|         7|         2|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Observo que el model no ha fet un bon clustering, ja que hi ha moltes observacions que no han quedat assignades al cluster que corresponen.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.clustering as cl\n",
    "\n",
    "# Creem un objecte de la classe KMeans\n",
    "kmeans = cl.KMeans(featuresCol='features', k=7)\n",
    "\n",
    "# Creem un objecte de la classe Pipeline\n",
    "pipeline_kmeans = Pipeline(stages=[assembler, kmeans])\n",
    "\n",
    "# Entrenem el model\n",
    "model_kmeans = pipeline_kmeans.fit(covertype_df)\n",
    "\n",
    "# Fem prediccions\n",
    "predictions_kmeans = model_kmeans.transform(covertype_df)\n",
    "\n",
    "# Mostrem les prediccions\n",
    "predictions_kmeans.select('Cover_Type', 'prediction').show()\n",
    "print(\"Observo que el model no ha fet un bon clustering, ja que hi ha moltes observacions que no han quedat assignades al cluster que corresponen.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
