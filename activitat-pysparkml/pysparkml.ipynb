{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llibreria ML de PySpark\n",
    "Part 1. Pràctica\n",
    "Descarrega: https://archive.ics.uci.edu/ml/datasets/covertype\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Carrega l’arxiu a un spark dataframe directament i carrega l’arxiu passant per un pas intermig de crear un format parquet i després el spark dataframe. Són els 2 dataframes iguals? Sí\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicialitzem spark\n",
    "findspark.init()\n",
    "\n",
    "# Creem una sessió de spark\n",
    "spark = SparkSession.builder.appName('pysparkml').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Afegim les capçaleres del dataset\n",
    "header = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Cover_Type']\n",
    "\n",
    "# Carreguem el dataset a un spark dataframe amb les capçaleres\n",
    "covertype_df = spark.read.csv('covtype.data', header=False, inferSchema=True)\n",
    "covertype_df = covertype_df.toDF(*header)\n",
    "\n",
    "# Convertim a parquet\n",
    "covertype_df.write.parquet('covertype.parquet', mode='overwrite')\n",
    "covertype_parquet_df = spark.read.parquet()\n",
    "\n",
    "# Comprovem que els dos dataframes són iguals\n",
    "covertype_df.count() == covertype_parquet_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Elimina un dels 2 dataframe i realitza un EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wilderness_Area1: integer (nullable = true)\n",
      " |-- Wilderness_Area2: integer (nullable = true)\n",
      " |-- Wilderness_Area3: integer (nullable = true)\n",
      " |-- Wilderness_Area4: integer (nullable = true)\n",
      " |-- Soil_Type1: integer (nullable = true)\n",
      " |-- Soil_Type2: integer (nullable = true)\n",
      " |-- Soil_Type3: integer (nullable = true)\n",
      " |-- Soil_Type4: integer (nullable = true)\n",
      " |-- Soil_Type5: integer (nullable = true)\n",
      " |-- Soil_Type6: integer (nullable = true)\n",
      " |-- Soil_Type7: integer (nullable = true)\n",
      " |-- Soil_Type8: integer (nullable = true)\n",
      " |-- Soil_Type9: integer (nullable = true)\n",
      " |-- Soil_Type10: integer (nullable = true)\n",
      " |-- Soil_Type11: integer (nullable = true)\n",
      " |-- Soil_Type12: integer (nullable = true)\n",
      " |-- Soil_Type13: integer (nullable = true)\n",
      " |-- Soil_Type14: integer (nullable = true)\n",
      " |-- Soil_Type15: integer (nullable = true)\n",
      " |-- Soil_Type16: integer (nullable = true)\n",
      " |-- Soil_Type17: integer (nullable = true)\n",
      " |-- Soil_Type18: integer (nullable = true)\n",
      " |-- Soil_Type19: integer (nullable = true)\n",
      " |-- Soil_Type20: integer (nullable = true)\n",
      " |-- Soil_Type21: integer (nullable = true)\n",
      " |-- Soil_Type22: integer (nullable = true)\n",
      " |-- Soil_Type23: integer (nullable = true)\n",
      " |-- Soil_Type24: integer (nullable = true)\n",
      " |-- Soil_Type25: integer (nullable = true)\n",
      " |-- Soil_Type26: integer (nullable = true)\n",
      " |-- Soil_Type27: integer (nullable = true)\n",
      " |-- Soil_Type28: integer (nullable = true)\n",
      " |-- Soil_Type29: integer (nullable = true)\n",
      " |-- Soil_Type30: integer (nullable = true)\n",
      " |-- Soil_Type31: integer (nullable = true)\n",
      " |-- Soil_Type32: integer (nullable = true)\n",
      " |-- Soil_Type33: integer (nullable = true)\n",
      " |-- Soil_Type34: integer (nullable = true)\n",
      " |-- Soil_Type35: integer (nullable = true)\n",
      " |-- Soil_Type36: integer (nullable = true)\n",
      " |-- Soil_Type37: integer (nullable = true)\n",
      " |-- Soil_Type38: integer (nullable = true)\n",
      " |-- Soil_Type39: integer (nullable = true)\n",
      " |-- Soil_Type40: integer (nullable = true)\n",
      " |-- Cover_Type: integer (nullable = true)\n",
      "\n",
      "+-------+-----------------+------------------+------------------+--------------------------------+------------------------------+-------------------------------+------------------+------------------+------------------+----------------------------------+------------------+--------------------+-------------------+-------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|summary|        Elevation|            Aspect|             Slope|Horizontal_Distance_To_Hydrology|Vertical_Distance_To_Hydrology|Horizontal_Distance_To_Roadways|     Hillshade_9am|    Hillshade_Noon|     Hillshade_3pm|Horizontal_Distance_To_Fire_Points|  Wilderness_Area1|    Wilderness_Area2|   Wilderness_Area3|   Wilderness_Area4|          Soil_Type1|         Soil_Type2|          Soil_Type3|          Soil_Type4|          Soil_Type5|          Soil_Type6|          Soil_Type7|          Soil_Type8|          Soil_Type9|         Soil_Type10|        Soil_Type11|        Soil_Type12|         Soil_Type13|         Soil_Type14|         Soil_Type15|        Soil_Type16|         Soil_Type17|         Soil_Type18|         Soil_Type19|         Soil_Type20|         Soil_Type21|        Soil_Type22|        Soil_Type23|        Soil_Type24|         Soil_Type25|         Soil_Type26|         Soil_Type27|         Soil_Type28|        Soil_Type29|        Soil_Type30|        Soil_Type31|        Soil_Type32|        Soil_Type33|         Soil_Type34|         Soil_Type35|         Soil_Type36|         Soil_Type37|         Soil_Type38|        Soil_Type39|        Soil_Type40|        Cover_Type|\n",
      "+-------+-----------------+------------------+------------------+--------------------------------+------------------------------+-------------------------------+------------------+------------------+------------------+----------------------------------+------------------+--------------------+-------------------+-------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|  count|           581012|            581012|            581012|                          581012|                        581012|                         581012|            581012|            581012|            581012|                            581012|            581012|              581012|             581012|             581012|              581012|             581012|              581012|              581012|              581012|              581012|              581012|              581012|              581012|              581012|             581012|             581012|              581012|              581012|              581012|             581012|              581012|              581012|              581012|              581012|              581012|             581012|             581012|             581012|              581012|              581012|              581012|              581012|             581012|             581012|             581012|             581012|             581012|              581012|              581012|              581012|              581012|              581012|             581012|             581012|            581012|\n",
      "|   mean|2959.365300544567|155.65680743254873|14.103703537964792|              269.42821662891646|            46.418855376481034|             2350.1466114297123|212.14604861861716|223.31871630878535|142.52826275533036|                1980.2912263430014|0.4488650836815763|0.051434393781884025| 0.4360736094951567|0.06362691304138296|0.005216759722690753|0.01295153972723455|0.008301033369362424|0.021335187569275677|0.002748652351414...|0.011316461622135171|1.807191589846681...|3.080831376976723...|0.001974141670051565|0.056167514612434855| 0.0213592834571403|0.05158413251361418|0.030001101526302382|0.001030959773636...|5.163404542419089E-6|0.00489662864106077|0.005889723448052708|0.003268435075351...|0.006920683221689053|0.015935987552752783|0.001442311002182399|0.05743943326471743|0.09939897971126242|0.03662230728453113|8.158179177022161E-4|0.004456018120107674|0.001869152444355...|0.001628193565709...|0.19835562776672427|0.05192663834826131|0.04417464699524278|0.09039228105443606|0.07771612290279718|0.002772748239279...|0.003254665996571...|2.048150468492905...|5.128981845469629E-4|0.026803232979697493|0.02376198770421265|0.01505992991538901| 2.051470537613681|\n",
      "| stddev|279.9847342506371|111.91372100329544| 7.488241814480139|              212.54935559508115|             58.29523162688723|             1559.2548698976084| 26.76988880528213|19.768697153666405| 38.27452923141059|                1324.1952097801075|0.4973787752982266|   0.220882278353401|0.49589700528224817|0.24408734404115698| 0.07203855962336306|0.11306555332463286| 0.09073114340173861| 0.14449925009498288| 0.05235553437289732|  0.1057753212014363|0.013441979413635834|0.017549608330903396| 0.04438747374857372| 0.23024512189599608|0.14457904566150795|0.22118610713609077|  0.1705904027792983| 0.03209203434149953|0.002272308686899177|0.06980444151712224| 0.07651826372232713| 0.05707677298648149| 0.08290234733979654| 0.12522802739167171| 0.03795040474080564|0.23268054915182676|0.29919722027093215|0.18783283689852426|0.028550897740997946| 0.06660457685375018| 0.04319330879373808|0.040318052398458246|0.39876176644617517|0.22187912770284737|0.20548313855427747| 0.2867431918891414|0.26772457957496393|  0.0525838840856219| 0.05695681459947709| 0.01430990741020341| 0.02264146643472847|  0.1615080944631041|0.15230691241832534|0.12179143630071344|1.3965043160794064|\n",
      "|    min|             1859|                 0|                 0|                               0|                          -173|                              0|                 0|                 0|                 0|                                 0|                 0|                   0|                  0|                  0|                   0|                  0|                   0|                   0|                   0|                   0|                   0|                   0|                   0|                   0|                  0|                  0|                   0|                   0|                   0|                  0|                   0|                   0|                   0|                   0|                   0|                  0|                  0|                  0|                   0|                   0|                   0|                   0|                  0|                  0|                  0|                  0|                  0|                   0|                   0|                   0|                   0|                   0|                  0|                  0|                 1|\n",
      "|    max|             3858|               360|                66|                            1397|                           601|                           7117|               254|               254|               254|                              7173|                 1|                   1|                  1|                  1|                   1|                  1|                   1|                   1|                   1|                   1|                   1|                   1|                   1|                   1|                  1|                  1|                   1|                   1|                   1|                  1|                   1|                   1|                   1|                   1|                   1|                  1|                  1|                  1|                   1|                   1|                   1|                   1|                  1|                  1|                  1|                  1|                  1|                   1|                   1|                   1|                   1|                   1|                  1|                  1|                 7|\n",
      "+-------+-----------------+------------------+------------------+--------------------------------+------------------------------+-------------------------------+------------------+------------------+------------------+----------------------------------+------------------+--------------------+-------------------+-------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminem el dataframe que no utilitzarem\n",
    "#covertype_parquet_df.unpersist()\n",
    "\n",
    "# Realitzem un EDA general del dataframe\n",
    "covertype_df.printSchema()\n",
    "covertype_df.describe().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Elimina duplicats si n’hi ha. Busca outliers i, si n’hi ha, tracta’ls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "+----------+------+\n",
      "|Cover_Type| count|\n",
      "+----------+------+\n",
      "|         1|211840|\n",
      "|         6| 17367|\n",
      "|         3| 35754|\n",
      "|         5|  9493|\n",
      "|         4|  2747|\n",
      "|         7| 20510|\n",
      "|         2|283301|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminem duplicats\n",
    "covertype_df = covertype_df.dropDuplicates()\n",
    "\n",
    "# Comprovem que no hi ha duplicats\n",
    "print(covertype_df.count() == covertype_df.dropDuplicates().count())\n",
    "\n",
    "# Eliminem nuls\n",
    "covertype_df = covertype_df.na.drop()\n",
    "\n",
    "# Nivellem la quantitat de Cover_Type per a que no hi hagi desequilibri en el model\n",
    "covertype_df.groupBy('Cover_Type').count().show()\n",
    "covertype_df = covertype_df.sampleBy('Cover_Type', fractions={1: 0.01, 2: 0.01, 3: 0.1, 4: 1, 5: 0.1, 6: 0.1, 7: 0.1}, seed=0)\n",
    "covertype_df.groupBy('Cover_Type').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Busquem outliers de cada columna\n",
    "def find_outliers(df, col):\n",
    "    # Calculem el quartil inferior\n",
    "    q1 = df.approxQuantile(col, [0.25], 0.0)[0]\n",
    "    # Calculem el quartil superior\n",
    "    q3 = df.approxQuantile(col, [0.75], 0.0)[0]\n",
    "    # Calculem el rang interquartil\n",
    "    iqr = q3 - q1\n",
    "    # Calculem el límit inferior\n",
    "    lower_bound = q1 - (1.5 * iqr)\n",
    "    # Calculem el límit superior\n",
    "    upper_bound = q3 + (1.5 * iqr)\n",
    "    # Retornem els outliers\n",
    "    return df.filter((df[col] < lower_bound) | (df[col] > upper_bound))\n",
    "\n",
    "# Busquem outliers de cada columna\n",
    "for col in covertype_df.columns:\n",
    "    print(col)\n",
    "    print(find_outliers(covertype_df, col).count())\n",
    "\n",
    "# Tractem els outliers de cada columna\n",
    "def treat_outliers(df, col):\n",
    "    # Calculem el quartil inferior\n",
    "    q1 = df.approxQuantile(col, [0.25], 0.0)[0]\n",
    "    # Calculem el quartil superior\n",
    "    q3 = df.approxQuantile(col, [0.75], 0.0)[0]\n",
    "    # Calculem el rang interquartil\n",
    "    iqr = q3 - q1\n",
    "    # Calculem el límit inferior\n",
    "    lower_bound = q1 - (1.5 * iqr)\n",
    "    # Calculem el límit superior\n",
    "    upper_bound = q3 + (1.5 * iqr)\n",
    "    # Retornem el dataframe sense outliers\n",
    "    return df.filter((df[col] >= lower_bound) & (df[col] <= upper_bound))\n",
    "\n",
    "# Tractem els outliers de cada columna\n",
    "for col in covertype_df.columns:\n",
    "    covertype_df = treat_outliers(covertype_df, col)\n",
    "\n",
    "# Comprovem que no hi ha outliers\n",
    "for col in covertype_df.columns:\n",
    "    print(col)\n",
    "    print(find_outliers(covertype_df, col).count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Problema de classificació de la variable COVER TYPE amb algoritme Regressió Logística del modul ML de PySpark.\n",
    "    Fixa’t en com es fa per agrupar i manipular les features a Spark.\n",
    "    Fixa’t en la transformació VectorAssembler.\n",
    "    Fixa’t amb la transformació Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6687138769835342\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.classification as cl\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Creem un vector de features\n",
    "feature_cols = covertype_df.columns[:-1]\n",
    "\n",
    "# Creem un objecte de la classe VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "\n",
    "# Creem un objecte de la classe LogisticRegression\n",
    "lr = cl.LogisticRegression(featuresCol='features', labelCol='Cover_Type', maxIter=10)\n",
    "\n",
    "# Creem un objecte de la classe Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "# Dividim el dataset en train i test\n",
    "train, test = covertype_df.randomSplit([0.7, 0.3], seed=1234)\n",
    "\n",
    "# Entrenem el model\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Fem prediccions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Mostrem l'accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Cover_Type', predictionCol='prediction', metricName='accuracy')\n",
    "print('Accuracy: ', evaluator.evaluate(predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Problema de classificació de la variable COVER TYPE amb algoritme Random forest amb modul ML de PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.666191729193717\n"
     ]
    }
   ],
   "source": [
    "# Creem un objecte de la classe RandomForestClassifier\n",
    "rf = cl.RandomForestClassifier(featuresCol='features', labelCol='Cover_Type', numTrees=10)\n",
    "\n",
    "# Creem un objecte de la classe Pipeline\n",
    "pipeline_rf = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Entrenem el model\n",
    "model_rf = pipeline_rf.fit(train)\n",
    "\n",
    "# Fem prediccions\n",
    "predictions_rf = model_rf.transform(test)\n",
    "\n",
    "# Mostrem l'accuracy\n",
    "print('Accuracy: ', evaluator.evaluate(predictions_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "MSE:  1.8837399027910238\n",
      "RMSE:  1.3724940447196934\n",
      "R2:  0.03746224907755458\n",
      "MAE:  0.5976283767479805\n",
      "Random Forest\n",
      "MSE:  2.0576417056383507\n",
      "RMSE:  1.434448223407994\n",
      "R2:  -0.051396648027091985\n",
      "MAE:  0.6203621781245332\n"
     ]
    }
   ],
   "source": [
    "# 6. Comparativa de les métriques d’error.\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Creem els evaluadors\n",
    "evaluator_mse = RegressionEvaluator(labelCol='Cover_Type', predictionCol='prediction', metricName='mse')\n",
    "evaluator_rmse = RegressionEvaluator(labelCol='Cover_Type', predictionCol='prediction', metricName='rmse')\n",
    "evaluator_r2 = RegressionEvaluator(labelCol='Cover_Type', predictionCol='prediction', metricName='r2')\n",
    "evaluator_mae = RegressionEvaluator(labelCol='Cover_Type', predictionCol='prediction', metricName='mae')\n",
    "\n",
    "# Comparem les métriques d'error dels dos models\n",
    "print('Logistic Regression')\n",
    "print('MSE: ', evaluator_mse.evaluate(predictions))\n",
    "print('RMSE: ', evaluator_rmse.evaluate(predictions))\n",
    "print('R2: ', evaluator_r2.evaluate(predictions))\n",
    "print('MAE: ', evaluator_mae.evaluate(predictions))\n",
    "print('Random Forest')\n",
    "print('MSE: ', evaluator_mse.evaluate(predictions_rf))\n",
    "print('RMSE: ', evaluator_rmse.evaluate(predictions_rf))\n",
    "print('R2: ', evaluator_r2.evaluate(predictions_rf))\n",
    "print('MAE: ', evaluator_mae.evaluate(predictions_rf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7. «Hyperparameters-tuning» amb modul ML.tuning de PySpark\n",
    "Necessites fer: import pyspark.ml.tuning\n",
    "Es proposa fer un «grid» al problema de classificació amb Regressió Logística.\n",
    "Fixa’t en regParam i en elasticNetParam per afegir rangs al objecte del «grid»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6655942272116191\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.tuning\n",
    "\n",
    "# Creem un objecte de la classe regParam\n",
    "regParam = pyspark.ml.tuning.ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1, 0.5]).build()\n",
    "\n",
    "# Creem un objecte de la classe elasticNetParam\n",
    "elasticNetParam = pyspark.ml.tuning.ParamGridBuilder().addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build()\n",
    "\n",
    "# Creem un objecte de la classe ParamGridBuilder\n",
    "grid = pyspark.ml.tuning.ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1, 0.5]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build()\n",
    "\n",
    "# Creem un objecte de la classe CrossValidator\n",
    "crossvalidator = pyspark.ml.tuning.CrossValidator(estimator=pipeline, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Entrenem el model\n",
    "model_cv = crossvalidator.fit(train)\n",
    "\n",
    "# Fem prediccions\n",
    "predictions_cv = model_cv.transform(test)\n",
    "\n",
    "# Mostrem l'accuracy\n",
    "print('Accuracy: ', evaluator.evaluate(predictions_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Problema de regressió de la variable ELEVATION amb algoritme Random forest regressor del modul ML de PySpark\n",
    "import pyspark.ml.regression as reg\n",
    "\n",
    "# Creem un objecte de la classe RandomForestRegressor\n",
    "rf = reg.RandomForestRegressor(featuresCol='features', labelCol='Elevation')\n",
    "\n",
    "# Creem un objecte de la classe Pipeline\n",
    "pipeline_rf = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Entrenem el model\n",
    "model_rf = pipeline_rf.fit(train)\n",
    "\n",
    "# Fem prediccions\n",
    "predictions_rf = model_rf.transform(test)\n",
    "\n",
    "# Mostrem l'accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "9. Problema de regressió de la variable ELEVATION amb algoritme Gradient-boosted Tree regressor del modul ML de PySpark\n",
    "\n",
    "10. Comparativa de les métriques d’error.\n",
    "\n",
    "11. Clustering\n",
    "import pyspark.ml.clustering\n",
    "Fer un k-means de 7 centroides del mateix DataFrame del inici de la pràctica, què observes?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
