{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creació d’un ResilientDistributedDataset o RDD des d’una llista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obrim Jupyter i completem el següent scrip, on el RDD es dirà myRDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ASIX', '20'), ('SMX', '19'), ('DAM', 17), ('DAW', '21')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import PySpark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName('ElNomDeLaMevaApp').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# Completa-ho, per exemple a ‘ASIX’ hi ha 20, ‘SMX’ hi ha19, ‘DAM’ hi ha 17 i ‘DAW’ hi ha 21, pista (Python List of arrays)\n",
    "data = [(\"ASIX\", \"20\"), (\"SMX\", \"19\"), (\"DAM\", 17), (\"DAW\", \"21\")]\n",
    "myRDD= sc.parallelize(data)\n",
    "# Visualitza-ho\n",
    "myRDD.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pregunta: Què significa «sc» davant del mètode «parallelize»?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Utilitzant aquest métode és pot distribuir myRDD en 4 nodes? Aquests 4 nodes operarien en paral·lel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si, No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Quin perill, en relació als «workers» i el «driver», té usar take() sense especificar el valor n en un RDD molt gran? Quines diferències hi ha amb el mètode «collect»?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que ens podem quedar sense memória, doncs el métode carrega tota la informació a la memòria del driver.\n",
    "Que el métode collect no permet seleccionar el límit d'elements a mostrar de la RDD i que el métode take escaneja la primera partició i utilitza els resultats per a estimar el número de particions necessaries per a satisfer el líímit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creació d’un ResilientDistributedDataset o RDD des d’un arxiu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obrim Jupyter i completem el següent scrip, on el RDD es dirà myRDD i llegirem un arxiu TXT o CSV que tindrem en local (ruta relativa o absoluta) i el text podrà estar separat per coma o per tabulador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['state', 'county', 'total_votes', 'dem_votes', 'rep_votes', 'other_votes', 'dem_share', 'east_west'], ['AK', '\"State House District 13', ' Greater Palmer\"', '11526', '2800', '8432', '294', '24.93', 'west'], ['AK', '\"State House District 16', ' Chugiak-South Mat-\"', '10697', '2636', '7774', '287', '25.32', 'west'], ['AK', '\"State House District 15', ' Rural Mat-Su\"', '11086', '2510', '8227', '349', '23.38', 'west'], ['AK', '\"State House District 2', ' Sitka-Wrangell-Pete\"', '7735', '3468', '4029', '238', '46.26', 'west']]\n",
      "3154\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName('ElNomDeLaMevaApp').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# Completa els 2 espais proposats\n",
    "myRDD= sc.textFile('2008_all_states.csv').map(lambda element: element.split(','))\n",
    "\n",
    "# Visualitza-ho\n",
    "print(myRDD.sample(True, 0.7).take(5))\n",
    "print(myRDD.count())\n",
    "print(myRDD.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Si usem el mètode «textFile» passant etiquetes «hdfs:», «s3:», «wasb:», «gs:» o «dbfs:» quin tips d’arxius estariem ingestant enlloc de fitxers locals? Especifíca les 5 respostes.\n",
    "1. hdfs - Fitxers de Hadoop\n",
    "2. s3 - Fitxers d'amazon cloud\n",
    "3. wasb - Fitxers d'azure\n",
    "4. gs - Fitxers de Google App Scripts\n",
    "5. dbfs - Fitxers de Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Si després del «take», afegim al script una línia més com la següent: myRDD.count() quin resultat estem observant?\n",
    "El número d'elements del RDD (157)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Si després del «count», afegim al script una línia més com la següent: myRDD.getNumPartitions() quin resultat estem observant?\n",
    "El número de particións (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Quina és la importància de «getNumPartitions()» ó dit d’una altra manera a què ens invita a comprobar?\n",
    "Ens permet veure el número de particions que ha creat el builder per a la RDD i ens convida a comprobar quants elements té cada partició"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Dins de «textFile» tenim que «minPartitions» és opcional, prova un valors diferent al que has posat en la primera pregunta i/o prova sense informar un valor per a «minPartitions», després comprova-ho amb «getNumPartitions()». Quan creus que és convenient que Spark optimitzi i quan és convenient que tu mateix especifíquis un mínim per al numero de particions?\n",
    "És convenient que optimitzi quan el volum de dades és dinàmic, i és convenient especificar quan es vol treballar amb el volum de dades de forma paralela amb unes particions previament especificades. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 7. Com més particions més paral·lelisme (sempre, quasi sempre,mai...raona)?\n",
    "Sempre, doncs el paral·lelisme aumenta per cada partició."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 8. Com més paral·lelisme més eficient és la «query» (sempre, quasi sempre,mai...raona)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si, Sempre que el número de particións no superi al número de blocs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 9. Dins de «textFile» tenim que «useUnicode» és opcional, si no especifiquem ni «useUnicode=True» ni «useUnicode=False» quina característica pren «textFile» per defecte?\n",
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 10. Ara treu-li la funció «map()» que està aplicada a «textFile», com canvia els resultats?\n",
    "El número d'elements i particións segueix sent el mateix, però el take mostra una sola llista sense separacións."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 11. Dins del codi de «map()», et recorda el terme lambda a les lambda de Python? Què és una funció anònima?\n",
    "És una funció especial sense nom de no més d'una línia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 12. Si a «Split» posem « \\t» quin és el separador? Pots fer servir el teu nom com a separador, si el tinguéssim al fitxer?\n",
    "El separador és cada línia del csv.\n",
    "Si tingués el meu nom, es faría una separació on estigués del fitxer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 13. Intenta agafar un fitxer molt gran i carrega’l varies vegades amb paràmetres tant del minPartitions com del map() que et donin joc (per defecte, grans, petits,...). La comanda myRDD.count() en algun moment t’informa de Output Duration en l’ordre de magnitud de segons? Et serveix per refinar el teu criteri d’ajust de paràmetres?\n",
    "Si afegeixo un nombre massa gran de particións, seveix per a reduir-ho i tenir un nombre de particións més òptim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 14. Per acabar, introduim el mètode «sample» i ho visualitazarem tot en una sola línia, es a dir, ...map(...).sample(...).take(...) «punt map punt sample punt take». El fitxer ha de ser prou gran i experimenta amb tots els paràmetres de «sample», entens quin efecte té cadascun? Quin valor prenen per defecte en cas que siguin opcionals? Quina utilitat té en grans volums de dades saber prendre mostres aleatories? Creus que «sample» et serveix per a les teves tasques d’analisi o hauries de codificar-te la teva propia funció?\n",
    "1. withReplacement - Els elements dins de la mostra es poden mostrar multiples vegades\n",
    "2. fraciton - Permet seleccionar el tamany de la fracció de la RDD entre 0 i 1\n",
    "3. seed - llavor per al generador de número aleatori\n",
    "És util utilitzar-ho en grans volums de dades per a obtenir diferents resultats alhora d'analitzar un dataframe i poder administrar millor temes com estadístiques...\n",
    "Si, serveix per a tasques d'anàlisi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 15. Dins PySpark hi ha també Filter, Select, Join, Distinct,...et son familiars, pots fer una comparativa amb altres llenguatges o llibreries on els utilitzes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principalment SQL i CQL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48ed48f5542fe07e8f725768fb6d295eb9c66c05001f1d4e389bf0879d8b38cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
